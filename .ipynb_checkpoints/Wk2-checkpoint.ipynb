{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(file):\n",
    "    clusters = []\n",
    "    with open(file, 'r') as f:\n",
    "        while True:\n",
    "            lines = f.readline()\n",
    "            if not lines:\n",
    "                break\n",
    "            else:\n",
    "                _, cluster = [int(i) for i in lines.split()]\n",
    "                clusters.append(cluster)\n",
    "    return clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_truth = read_txt('partitions.txt')\n",
    "cluster_1 = read_txt('clustering_1.txt')\n",
    "cluster_2 = read_txt('clustering_2.txt')\n",
    "cluster_3 = read_txt('clustering_3.txt')\n",
    "cluster_4 = read_txt('clustering_4.txt')\n",
    "cluster_5 = read_txt('clustering_5.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP, FN, FP = 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jac_sim(cluster_truth, cluster_i):\n",
    "    TP = FN = FP = 0\n",
    "    for i, j in combinations(range(len(cluster_truth)), 2):\n",
    "        comparision1 = cluster_truth[i] == cluster_truth[j]\n",
    "        comparision2 = cluster_i[i] == cluster_i[j]\n",
    "        if comparision1 and comparision2:\n",
    "            TP += 1\n",
    "        elif comparision1 and not comparision2:\n",
    "            FN += 1\n",
    "        elif comparision2 and not comparision1:\n",
    "            FP += 1\n",
    "    return TP/(TP+FN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8005979461848434"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = jac_sim(cluster_truth, cluster_4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.jaccard_similarity_score(cluster_truth, cluster_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\cluster\\supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "with open('scores.txt', 'w') as f:\n",
    "    for cluster_i in [cluster_1, cluster_2, cluster_3, cluster_4, cluster_5]:\n",
    "        nmi = metrics.normalized_mutual_info_score(cluster_truth, cluster_i)\n",
    "        jac = jac_sim(cluster_truth, cluster_i)\n",
    "        f.write(f'{nmi:8.7f} {jac:8.7f}\\n')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
